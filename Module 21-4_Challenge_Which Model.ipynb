{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Which model?\n",
    "You now have a fairly substantial starting toolbox of supervised learning methods that you can use to tackle a host of exciting problems. To make sure all of these ideas are organized in your mind, please go through the list of problems below. For each, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor.\n",
    "\n",
    "1) Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "    - Linear regression models can predict on data with a normal distribution of data that fit a trend. \n",
    "    - Locally Weighted Linear Regression, using Gaussian kernal because literature suggests best error rate predictions.\n",
    "    - Sprints generally lie within a small range of data and we trying to explain 1 outcome for 1 type of race.\n",
    "    - Knowing sprints are generally measured in seconds, outliers should be easy to tackle.  \n",
    "   \n",
    "\n",
    "2) You have more features (columns) than rows in your dataset.\n",
    "    - Feature reduction methods, like PCA or PLS, combining features, eliminate features with low correlation with target, reducing multicolinear/colinear relationships. Not all of these steps are always necessary but at least good to keep on hand for brain storming.\n",
    "\n",
    "3) Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
    "    - In a gradient boosting or decision tree model, you can run a feature importance script, in any model, you can utilize a correlation matrix.\n",
    "\n",
    "4) Implement a filter to “highlight” emails that might be important to the recipient.\n",
    "    - Any classfier model will do here, but more robust models include gradient boosting, KNN, SVM, decision trees, and random forests.\n",
    "    - In this model, it would be good to utilize create dummy columns of each word, create other stats like word length and ranks.\n",
    "\n",
    "5) You have 1000+ features.\n",
    "    - Determine scope of the question\n",
    "    - https://www.codementor.io/@guidotournois/4-strategies-to-deal-with-large-datasets-using-pandas-qdw3an95k helps with this discussion\n",
    "    - if machine runs out of memory, open csv with chunksize, iterate each chunk with chunk filter, and/or sample dataframe with a percentage of the data.\n",
    "    - run logistic regression \n",
    "    - filter out any columns that might not be labeled, or deemed unimportant by subject matter expert\n",
    "    - optimize dypes to decrease memory requirements (int64 to in32) if needed\n",
    "\n",
    "6) Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "    - Using Classification algorithim\n",
    "    - features to look for: number of items in cart, item, purchased?, date purchased, date put in cart, length of time in cart, itme price, item review scores\n",
    "    - get dummies on object columns\n",
    "    - split data\n",
    "    - run accuracy per classification model to get best performer (cross_val_score, kfold, scoring)\n",
    "    - tune parameters\n",
    "\n",
    "7) Your dataset dimensions are 982400 x 500\n",
    "    - similar to answer 5, consider starting with smaller sample of the data first\n",
    "    - run logistic regression \n",
    "    - filter out poor-performing columns\n",
    "    - optimize dtype memory \n",
    "    - clean data\n",
    "    - remove variables with high colinear/multilinear relationships\n",
    "    - in a random forest or gradient boosting model, fit sample, find features of importance\n",
    "\n",
    "8) Identify faces in an image.\n",
    "https://www.kaggle.com/serkanpeldek/face-recognition-on-olivetti-dataset\n",
    "    - split data \n",
    "    - run PCA on features, find best components\n",
    "    - train model/predict with SVC\n",
    "    - rank against other models for best accuracy\n",
    "    - validate results using cross_val/kfold\n",
    "    - attempt the leave one out cross val method\n",
    "    - hyperparam tuning using gridsearch\n",
    "    - precision-recall-ROC Curves\n",
    "\n",
    "9) Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
    "    - using classifier\n",
    "    - data include: flavor, gender, flavor ranks, age\n",
    "    - subset data based on age (likely t-test)\n",
    "    - dummies on object cols\n",
    "    - train/split data, target is gender\n",
    "    - find features of importance\n",
    "    - cross validate\n",
    "    - insert rest of data to see if the trend is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
