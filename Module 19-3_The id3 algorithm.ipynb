{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Entropy\n",
    "\n",
    "Since this algorithm is based on entropy let's go over a quick example of how to calculate it.\n",
    "\n",
    "Let's say we have 20 students, and we're trying to classify them as male and female. The only attribute we have is whether their height is tall, medium, or short. Of the 20 students, 12 are boys and 8 are girls. Of the 12 boys, 4 are tall, 6 are medium, and 2 are short. Of the 8 girls, 1 is tall, 2 are medium, and 5 are short.\n",
    "\n",
    "What is the entropy, both before any rule is applied and then after applying a rule for being tall?\n",
    "\n",
    "The initial entropy is just the formula plugged in over both the possible classes of interest:\n",
    "\n",
    "$$ H = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "\n",
    "$$ = \\frac{12}{20}*log_2 \\frac{20}{12} + \\frac{8}{20}*log_2 \\frac{20}{8} = .971 $$\n",
    "\n",
    "What if we apply the rule _height = short_? We need to calculate the weighted average of the two entropies, one for the short students and one for the non-short students.\n",
    "\n",
    "$$ H(short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{2}{7}*log_2 \\frac{7}{2} + \\frac{5}{7}*log_2 \\frac{7}{5} = .863 $$\n",
    "\n",
    "$$ H(not\\_short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{10}{13}*log_2 \\frac{13}{10} + \\frac{3}{13}*log_2 \\frac{13}{3} = .779 $$\n",
    "\n",
    "Note that all the probabilities here are conditional on the criteria we're assuming (short or not short). Now the weighted average of the two is just:\n",
    "\n",
    "$$ P(short) * H(short) + P(not\\_short) * H(not\\_short) = \\frac{7}{20} * .863 + \\frac{13}{20} * .779 = .809 $$\n",
    "\n",
    "So our entropy from this question would go from .971 to .809. That's an improvement. Use the space below to calculate the entropy of the other criteria, for we asked whether the students were tall or medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "# Put your calculations below\n",
    "# tall\n",
    "p_tall = 5/20\n",
    "h_tall = ((4/5) * log2(5/4)) + ((1/5) * log2(5/1))\n",
    "p_not_tall = 15/20\n",
    "h_not_tall = ((8/15) * log2(15/8)) + ((7/15) * log2(15/7))\n",
    "\n",
    "entropy_tall = p_tall * h_tall + p_not_tall * h_not_tall\n",
    "print('entropy_tall is ' + str(entropy_tall))\n",
    "\n",
    "# medium\n",
    "p_medium = 8/20\n",
    "h_medium = ((6/8) * log2(8/6)) + ((2/8) * log2(8/2))\n",
    "p_not_medium = 12/20\n",
    "h_not_medium = ((6/12) * log2(12/6)) + ((6/12) * log2(12/6))\n",
    "                                        \n",
    "entropy_medium = p_medium * h_medium + p_not_medium * h_not_medium\n",
    "print('entropy_medium is ' + str(entropy_medium))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching real ID3 code with pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocoding the Algorithm\n",
    "Pseudocode is the processes of writing the steps and logic you would implement in code, but in normal language rather than commands a programming language could execute. It can be a useful way to chart out how you want to build an algorithm and is a common topic for technical interviews. Here we'll use pseudocode to explain the ID3 algorithm.\n",
    "\n",
    "Here is reasonable pseudocode for ID3, which we'll then follow up with an explanation of the steps. The outcome for this variable will be A or B. An attribute is denoted ai. A value of that attribute is vi.\n",
    "\n",
    "<pre>\n",
    "Algorithm(Observations, Outcome, Attributes)\n",
    "    Create a root node.\n",
    "    If all observations are 'A', label root node 'A' and return.\n",
    "    If all observations are 'B', label root node 'B' and return.\n",
    "    If no attributes return the root note labeled with the most common Outcome.\n",
    "    Otherwise, start:\n",
    "        For each value v<sub>i</sub> of each attribute a<sub>i</sub>, calculate the entropy.\n",
    "        The attribute a<sub>i</sub> and value v<sub>i</sub> with the lowest entropy is the best rule.\n",
    "        The attribute for this node is then a<sub>i</sub>\n",
    "            Split the tree to below based on the rule a<sub>i</sub> = v<sub>i</sub>\n",
    "            Observations<sub>v<sub>i</sub></sub> is the subset of observations with value v<sub>i</sub>\n",
    "            If Observations<sub>v<sub>i</sub></sub> is empty cap with node labeled with most common Outcome\n",
    "            Else at the new node start a subtree (Observations<sub>v<sub>i</sub></sub>, Target Outcome, Attributes - {a<sub>i</sub>}) and repeat the algorithm\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRILL:\n",
    "\n",
    "Look over the code for [this real](https://github.com/NinjaSteph/DecisionTree/blob/master/sna2111_DecisionTree/DecisionTree.py) ID3 Algorithm in python. Note how well the author breaks up functionality into individual, reusable, well-named helper functions. See if you can match our pseudocode steps to the code in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#find item in a list\n",
    "def find(item, list):\n",
    "    for i in list:\n",
    "        if item(i): \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "#find most common value for an attribute\n",
    "\n",
    "#Algorithm(Observations, Outcome, Attributes)\n",
    "    #Create a root node.\n",
    "    #If all observations are 'A', label root node 'A' and return.\n",
    "    #If all observations are 'B', label root node 'B' and return.\n",
    "    #If no attributes return the root note labeled with the most common Outcome.\n",
    "\n",
    "def majority(attributes, data, target):\n",
    "    #find target attribute\n",
    "    valFreq = {}\n",
    "    #find target in data\n",
    "    index = attributes.index(target)\n",
    "    #calculate frequency of values in target attr\n",
    "    for tuple in data:\n",
    "        if (valFreq.has_key(tuple[index])):\n",
    "            valFreq[tuple[index]] += 1 \n",
    "        else:\n",
    "            valFreq[tuple[index]] = 1\n",
    "    max = 0\n",
    "    major = \"\"\n",
    "    for key in valFreq.keys():\n",
    "        if valFreq[key]>max:\n",
    "            max = valFreq[key]\n",
    "            major = key\n",
    "    return major\n",
    "\n",
    "#Calculates the entropy of the given data set for the target attr\n",
    "\n",
    "#Otherwise, start:\n",
    "  #      For each value vi of each attribute ai, calculate the entropy.\n",
    "def entropy(attributes, data, targetAttr):\n",
    "\n",
    "    valFreq = {}\n",
    "    dataEntropy = 0.0\n",
    "    \n",
    "    #find index of the target attribute\n",
    "    i = 0\n",
    "    for entry in attributes:\n",
    "        if (targetAttr == entry):\n",
    "            break\n",
    "        ++i\n",
    "    \n",
    "    # Calculate the frequency of each of the values in the target attr\n",
    "    for entry in data:\n",
    "        if (valFreq.has_key(entry[i])):\n",
    "            valFreq[entry[i]] += 1.0\n",
    "        else:\n",
    "            valFreq[entry[i]]  = 1.0\n",
    "\n",
    "    # Calculate the entropy of the data for the target attr\n",
    "    for freq in valFreq.values():\n",
    "        dataEntropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    "        \n",
    "    return dataEntropy\n",
    "\n",
    "def gain(attributes, data, attr, targetAttr):\n",
    "    \"\"\"\n",
    "    Calculates the information gain (reduction in entropy) that would\n",
    "    result by splitting the data on the chosen attribute (attr).\n",
    "    \"\"\"\n",
    "    valFreq = {}\n",
    "    subsetEntropy = 0.0\n",
    "    \n",
    "    #find index of the attribute\n",
    "    i = attributes.index(attr)\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for entry in data:\n",
    "        if (valFreq.has_key(entry[i])):\n",
    "            valFreq[entry[i]] += 1.0\n",
    "        else:\n",
    "            valFreq[entry[i]]  = 1.0\n",
    "    # Calculate the sum of the entropy for each subset of records weighted\n",
    "    # by their probability of occuring in the training set.\n",
    "    for val in valFreq.keys():\n",
    "        valProb        = valFreq[val] / sum(valFreq.values())\n",
    "        dataSubset     = [entry for entry in data if entry[i] == val]\n",
    "        subsetEntropy += valProb * entropy(attributes, dataSubset, targetAttr)\n",
    "\n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the\n",
    "    # whole data set with respect to the target attribute (and return it)\n",
    "    return (entropy(attributes, data, targetAttr) - subsetEntropy)\n",
    "\n",
    "\n",
    " #The attribute ai and value vi with the lowest entropy is the best rule.\n",
    "#choose best attibute \n",
    "def chooseAttr(data, attributes, target):\n",
    "    best = attributes[0]\n",
    "    maxGain = 0;\n",
    "    for attr in attributes:\n",
    "        newGain = gain(attributes, data, attr, target) \n",
    "        if newGain>maxGain:\n",
    "            maxGain = newGain\n",
    "            best = attr\n",
    "    return best\n",
    "\n",
    "#get values in the column of the given attribute \n",
    "def getValues(data, attributes, attr):\n",
    "    index = attributes.index(attr)\n",
    "    values = []\n",
    "    for entry in data:\n",
    "        if entry[index] not in values:\n",
    "            values.append(entry[index])\n",
    "    return values\n",
    "\n",
    "#The attribute for this node is then ai\n",
    "  #          Split the tree to below based on the rule ai = vi\n",
    "def getExamples(data, attributes, best, val):\n",
    "    examples = [[]]\n",
    "    index = attributes.index(best)\n",
    "    for entry in data:\n",
    "        #find entries with the give value\n",
    "        if (entry[index] == val):\n",
    "            newEntry = []\n",
    "            #add value if it is not in best column\n",
    "            for i in range(0,len(entry)):\n",
    "                if(i != index):\n",
    "                    newEntry.append(entry[i])\n",
    "            examples.append(newEntry)\n",
    "    examples.remove([])\n",
    "    return examples\n",
    "\n",
    "def makeTree(data, attributes, target, recursion):\n",
    "    recursion += 1\n",
    "    #Returns a new decision tree based on the examples given.\n",
    "    data = data[:]\n",
    "    vals = [record[attributes.index(target)] for record in data]\n",
    "    default = majority(attributes, data, target)\n",
    "\n",
    "    # If the dataset is empty or the attributes list is empty, return the\n",
    "    # default value. When checking the attributes list for emptiness, we\n",
    "    # need to subtract 1 to account for the target attribute.\n",
    "    if not data or (len(attributes) - 1) <= 0:\n",
    "        return default\n",
    "    # If all the records in the dataset have the same classification,\n",
    "    # return that classification.\n",
    "    elif vals.count(vals[0]) == len(vals):\n",
    "        return vals[0]\n",
    "    else:\n",
    "        # Choose the next best attribute to best classify our data\n",
    "        best = chooseAttr(data, attributes, target)\n",
    "        # Create a new decision tree/node with the best attribute and an empty\n",
    "        # dictionary object--we'll fill that up next.\n",
    "        tree = {best:{}}\n",
    "    \n",
    "        # Create a new decision tree/sub-node for each of the values in the\n",
    "        # best attribute field\n",
    "        for val in getValues(data, attributes, best):\n",
    "            # Create a subtree for the current value under the \"best\" field\n",
    "            examples = getExamples(data, attributes, best, val)\n",
    "            newAttr = attributes[:]\n",
    "            newAttr.remove(best)\n",
    "            subtree = makeTree(examples, newAttr, target, recursion)\n",
    "  #  Else at the new node start a subtree (Observationsvi, Target Outcome, Attributes - {ai}) and repeat the algorithm\n",
    "            # Add the new subtree to the empty dictionary object in our new\n",
    "            # tree/node we just created.\n",
    "            tree[best][val] = subtree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
